15:28:02,871 graphrag.index.cli INFO Logging enabled at D:\CloudNative\AIOps\Camp\module_03\demo_08\data\output\indexing-engine.log
15:28:02,873 graphrag.index.cli INFO Starting pipeline run for: 20240921-152802, dryrun=False
15:28:02,873 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://api.apiyi.com/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "D:\\CloudNative\\AIOps\\Camp\\module_03\\demo_08\\data",
    "reporting": {
        "type": "file",
        "base_dir": "D:\\CloudNative\\AIOps\\Camp\\module_03\\demo_08\\data\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "D:\\CloudNative\\AIOps\\Camp\\module_03\\demo_08\\data\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
15:28:02,886 graphrag.index.create_pipeline_config INFO skipping workflows 
15:28:02,886 graphrag.index.run.run INFO Running pipeline
15:28:02,886 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at D:\CloudNative\AIOps\Camp\module_03\demo_08\data\output
15:28:02,888 graphrag.index.input.load_input INFO loading input from root_dir=input
15:28:02,888 graphrag.index.input.load_input INFO using file storage for input
15:28:02,888 graphrag.index.storage.file_pipeline_storage INFO search D:\CloudNative\AIOps\Camp\module_03\demo_08\data\input for files matching .*\.txt$
15:28:02,889 graphrag.index.input.text INFO found text files from input, found [('data.txt', {})]
15:28:02,889 graphrag.index.input.text INFO Found 1 files, loading 1
15:28:02,891 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
15:28:02,891 graphrag.index.run.run INFO Final # of rows loaded: 1
15:28:02,969 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
15:28:02,972 datashaper.workflow.workflow INFO executing verb orderby
15:28:02,974 datashaper.workflow.workflow INFO executing verb zip
15:28:02,976 datashaper.workflow.workflow INFO executing verb aggregate_override
15:28:02,981 datashaper.workflow.workflow INFO executing verb chunk
15:28:03,86 datashaper.workflow.workflow INFO executing verb select
15:28:03,90 datashaper.workflow.workflow INFO executing verb unroll
15:28:03,92 datashaper.workflow.workflow INFO executing verb rename
15:28:03,94 datashaper.workflow.workflow INFO executing verb genid
15:28:03,97 datashaper.workflow.workflow INFO executing verb unzip
15:28:03,101 datashaper.workflow.workflow INFO executing verb copy
15:28:03,103 datashaper.workflow.workflow INFO executing verb filter
15:28:03,121 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
15:28:03,210 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
15:28:03,210 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
15:28:03,227 datashaper.workflow.workflow INFO executing verb entity_extract
15:28:03,229 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://api.apiyi.com/v1
15:28:03,757 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=0, RPM=0
15:28:03,758 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
15:28:10,832 httpx INFO HTTP Request: POST https://api.apiyi.com/v1/chat/completions "HTTP/1.1 200 OK"
15:28:10,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.077999999994063. input_tokens=2250, output_tokens=642
15:28:26,68 httpx INFO HTTP Request: POST https://api.apiyi.com/v1/chat/completions "HTTP/1.1 200 OK"
15:28:26,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.235000000000582. input_tokens=34, output_tokens=379
15:28:26,77 datashaper.workflow.workflow INFO executing verb merge_graphs
15:28:26,81 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
15:28:26,168 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
15:28:26,169 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
15:28:26,182 datashaper.workflow.workflow INFO executing verb summarize_descriptions
15:28:26,186 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
15:28:26,269 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
15:28:26,270 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
15:28:26,279 datashaper.workflow.workflow INFO executing verb cluster_graph
15:28:26,289 datashaper.workflow.workflow INFO executing verb select
15:28:26,290 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
15:28:26,377 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
15:28:26,377 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:28:26,391 datashaper.workflow.workflow INFO executing verb unpack_graph
15:28:26,396 datashaper.workflow.workflow INFO executing verb rename
15:28:26,400 datashaper.workflow.workflow INFO executing verb select
15:28:26,404 datashaper.workflow.workflow INFO executing verb dedupe
15:28:26,408 datashaper.workflow.workflow INFO executing verb rename
15:28:26,413 datashaper.workflow.workflow INFO executing verb filter
15:28:26,425 datashaper.workflow.workflow INFO executing verb text_split
15:28:26,430 datashaper.workflow.workflow INFO executing verb drop
15:28:26,435 datashaper.workflow.workflow INFO executing verb merge
15:28:26,441 datashaper.workflow.workflow INFO executing verb text_embed
15:28:26,442 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
15:28:26,963 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
15:28:26,963 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
15:28:26,964 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 14 inputs via 14 snippets using 1 batches. max_batch_size=16, max_tokens=8191
15:28:28,122 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 401 Unauthorized"
15:28:28,124 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['PAYMENT SYSTEM:The payment system is a structured framework for processing transactions, including components like payment gateways, databases, and application servers.', 'OPERATIONS TEAM:The operations team is responsible for the maintenance and support of the payment system, providing assistance and troubleshooting.', 'PROMETHEUS:Prometheus is a monitoring tool used to track the performance and health of the payment system.', 'GRAFANA:Grafana is a visualization tool that works with Prometheus to display metrics and alerts for the payment system.', 'SYSTEM DOWNTIME:System downtime refers to periods when the payment system is unavailable, impacting transaction processing.', 'PAYMENT FAILURE:Payment failure occurs when transactions do not complete successfully due to various issues.', 'DATA ENCRYPTION:Data encryption is a security practice involving the use of TLS and AES to protect sensitive information in the payment system.', 'BACKUP AND RECOVERY:Backup and recovery procedures are critical processes for restoring the payment system in case of failure.', 'LOAD TESTING:Load testing is an evaluation method used to assess the performance of the payment system under various conditions.', 'COMMON ISSUES AND SOLUTIONS:Common issues and solutions provide guidance on troubleshooting problems that may arise within the payment system.', 'MONITORING AND ALERTS:Monitoring and alerts involve tracking system performance and setting up notifications for critical issues.', 'SECURITY BEST PRACTICES:Security best practices outline measures to protect the payment system from vulnerabilities and threats.', 'PERFORMANCE OPTIMIZATION:Performance optimization includes strategies to enhance the efficiency and speed of the payment system.', 'SYSTEM ARCHITECTURE:']}
15:28:28,124 datashaper.workflow.workflow ERROR Error executing verb "text_embed" in create_final_entities: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-NUkr3***************************************2b21. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Traceback (most recent call last):
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 62, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 106, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 100, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\openai\openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\resources\embeddings.py", line 237, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py", line 1821, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py", line 1515, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py", line 1616, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-NUkr3***************************************2b21. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
15:28:28,142 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "text_embed" in create_final_entities: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-NUkr3***************************************2b21. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}} details=None
15:28:28,147 graphrag.index.run.run ERROR error running workflow create_final_entities
Traceback (most recent call last):
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\run\run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\run\workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\datashaper\workflow\workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\datashaper\workflow\workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 62, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 106, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\verbs\text\embed\strategies\openai.py", line 100, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\base\base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\llm\openai\openai_embeddings_llm.py", line 36, in _execute_llm
    embedding = await self.client.embeddings.create(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\resources\embeddings.py", line 237, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py", line 1821, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py", line 1515, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hiron\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py", line 1616, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-NUkr3***************************************2b21. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
15:28:28,149 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
15:28:28,164 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
15:30:06,394 graphrag.index.cli INFO Logging enabled at D:\CloudNative\AIOps\Camp\module_03\demo_08\data\output\indexing-engine.log
15:30:06,396 graphrag.index.cli INFO Starting pipeline run for: 20240921-153006, dryrun=False
15:30:06,397 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o-mini",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://api.apiyi.com/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "D:\\CloudNative\\AIOps\\Camp\\module_03\\demo_08\\data",
    "reporting": {
        "type": "file",
        "base_dir": "D:\\CloudNative\\AIOps\\Camp\\module_03\\demo_08\\data\\output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "D:\\CloudNative\\AIOps\\Camp\\module_03\\demo_08\\data\\output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o-mini",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.apiyi.com/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
15:30:06,398 graphrag.index.create_pipeline_config INFO skipping workflows 
15:30:06,398 graphrag.index.run.run INFO Running pipeline
15:30:06,399 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at D:\CloudNative\AIOps\Camp\module_03\demo_08\data\output
15:30:06,399 graphrag.index.input.load_input INFO loading input from root_dir=input
15:30:06,399 graphrag.index.input.load_input INFO using file storage for input
15:30:06,400 graphrag.index.storage.file_pipeline_storage INFO search D:\CloudNative\AIOps\Camp\module_03\demo_08\data\input for files matching .*\.txt$
15:30:06,400 graphrag.index.input.text INFO found text files from input, found [('data.txt', {})]
15:30:06,401 graphrag.index.input.text INFO Found 1 files, loading 1
15:30:06,402 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
15:30:06,402 graphrag.index.run.run INFO Final # of rows loaded: 1
15:30:06,479 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
15:30:06,482 datashaper.workflow.workflow INFO executing verb orderby
15:30:06,484 datashaper.workflow.workflow INFO executing verb zip
15:30:06,486 datashaper.workflow.workflow INFO executing verb aggregate_override
15:30:06,489 datashaper.workflow.workflow INFO executing verb chunk
15:30:06,588 datashaper.workflow.workflow INFO executing verb select
15:30:06,591 datashaper.workflow.workflow INFO executing verb unroll
15:30:06,594 datashaper.workflow.workflow INFO executing verb rename
15:30:06,597 datashaper.workflow.workflow INFO executing verb genid
15:30:06,599 datashaper.workflow.workflow INFO executing verb unzip
15:30:06,602 datashaper.workflow.workflow INFO executing verb copy
15:30:06,605 datashaper.workflow.workflow INFO executing verb filter
15:30:06,611 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
15:30:06,701 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
15:30:06,702 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
15:30:06,716 datashaper.workflow.workflow INFO executing verb entity_extract
15:30:06,718 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://api.apiyi.com/v1
15:30:07,239 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o-mini: TPM=0, RPM=0
15:30:07,239 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o-mini: 25
15:30:07,263 datashaper.workflow.workflow INFO executing verb merge_graphs
15:30:07,266 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
15:30:07,353 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
15:30:07,353 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
15:30:07,363 datashaper.workflow.workflow INFO executing verb summarize_descriptions
15:30:07,368 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
15:30:07,457 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
15:30:07,457 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
15:30:07,468 datashaper.workflow.workflow INFO executing verb cluster_graph
15:30:07,477 datashaper.workflow.workflow INFO executing verb select
15:30:07,479 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
15:30:07,568 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
15:30:07,568 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:30:07,583 datashaper.workflow.workflow INFO executing verb unpack_graph
15:30:07,588 datashaper.workflow.workflow INFO executing verb rename
15:30:07,592 datashaper.workflow.workflow INFO executing verb select
15:30:07,597 datashaper.workflow.workflow INFO executing verb dedupe
15:30:07,602 datashaper.workflow.workflow INFO executing verb rename
15:30:07,606 datashaper.workflow.workflow INFO executing verb filter
15:30:07,618 datashaper.workflow.workflow INFO executing verb text_split
15:30:07,624 datashaper.workflow.workflow INFO executing verb drop
15:30:07,629 datashaper.workflow.workflow INFO executing verb merge
15:30:07,635 datashaper.workflow.workflow INFO executing verb text_embed
15:30:07,636 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://api.apiyi.com/v1
15:30:08,162 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
15:30:08,162 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
15:30:08,162 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 14 inputs via 14 snippets using 1 batches. max_batch_size=16, max_tokens=8191
15:30:10,812 httpx INFO HTTP Request: POST https://api.apiyi.com/v1/embeddings "HTTP/1.1 200 OK"
15:30:10,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.7189999999973224. input_tokens=305, output_tokens=0
15:30:10,905 datashaper.workflow.workflow INFO executing verb drop
15:30:10,910 datashaper.workflow.workflow INFO executing verb filter
15:30:10,920 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
15:30:11,24 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
15:30:11,24 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:30:11,37 datashaper.workflow.workflow INFO executing verb layout_graph
15:30:11,47 datashaper.workflow.workflow INFO executing verb unpack_graph
15:30:11,54 datashaper.workflow.workflow INFO executing verb unpack_graph
15:30:11,60 datashaper.workflow.workflow INFO executing verb filter
15:30:11,75 datashaper.workflow.workflow INFO executing verb drop
15:30:11,82 datashaper.workflow.workflow INFO executing verb select
15:30:11,88 datashaper.workflow.workflow INFO executing verb rename
15:30:11,94 datashaper.workflow.workflow INFO executing verb convert
15:30:11,123 datashaper.workflow.workflow INFO executing verb join
15:30:11,134 datashaper.workflow.workflow INFO executing verb rename
15:30:11,136 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
15:30:11,235 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
15:30:11,235 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:30:11,252 datashaper.workflow.workflow INFO executing verb create_final_communities
15:30:11,261 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
15:30:11,358 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
15:30:11,358 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
15:30:11,362 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
15:30:11,384 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
15:30:11,392 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
15:30:11,397 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
15:30:11,496 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
15:30:11,498 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
15:30:11,500 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
15:30:11,509 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
15:30:11,538 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
15:30:11,552 datashaper.workflow.workflow INFO executing verb select
15:30:11,554 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
15:30:11,651 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
15:30:11,651 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
15:30:11,654 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
15:30:11,673 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
15:30:11,682 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
15:30:11,691 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
15:30:11,702 datashaper.workflow.workflow INFO executing verb prepare_community_reports
15:30:11,702 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 14
15:30:11,719 datashaper.workflow.workflow INFO executing verb create_community_reports
15:30:20,578 httpx INFO HTTP Request: POST https://api.apiyi.com/v1/chat/completions "HTTP/1.1 200 OK"
15:30:20,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.85899999999674. input_tokens=2644, output_tokens=966
15:30:20,606 datashaper.workflow.workflow INFO executing verb window
15:30:20,607 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
15:30:20,717 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
15:30:20,725 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
15:30:20,752 datashaper.workflow.workflow INFO executing verb unroll
15:30:20,763 datashaper.workflow.workflow INFO executing verb select
15:30:20,773 datashaper.workflow.workflow INFO executing verb rename
15:30:20,784 datashaper.workflow.workflow INFO executing verb join
15:30:20,796 datashaper.workflow.workflow INFO executing verb aggregate_override
15:30:20,807 datashaper.workflow.workflow INFO executing verb join
15:30:20,821 datashaper.workflow.workflow INFO executing verb rename
15:30:20,833 datashaper.workflow.workflow INFO executing verb convert
15:30:20,846 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
15:30:20,947 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
15:30:20,947 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
15:30:20,973 datashaper.workflow.workflow INFO executing verb rename
15:30:20,975 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
15:30:21,11 graphrag.index.cli INFO All workflows completed successfully.
